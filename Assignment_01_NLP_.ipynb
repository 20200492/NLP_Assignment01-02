{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6SUQjsmQDy9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: URL to extract HTML from\n",
        "url = \"https://en.wikipedia.org/wiki/Mohamed_Salah\"\n",
        "\n",
        "# Step 2: Extract HTML from URL\n",
        "response = requests.get(url)\n",
        "html = response.text"
      ],
      "metadata": {
        "id": "E4ZI5LDIQQHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Extract text from HTML page (paragraphs and headings)\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "# Extract paragraphs (p tags)\n",
        "paragraphs = soup.find_all(\"p\")\n",
        "paragraph_text = [p.get_text() for p in paragraphs]\n",
        "\n",
        "# Extract headings (h1, h2, h3, etc. tags)\n",
        "headings = soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
        "heading_text = [heading.get_text() for heading in headings]\n",
        "\n",
        "# Combine paragraph and heading text\n",
        "combined_text = paragraph_text + heading_text\n",
        "\n",
        "# Print the combined text\n",
        "for text in combined_text:\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "IQVLOtvgQaDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "tO60uCEFSP45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Cleaning data\n",
        "cleaned_text = [re.sub(r'[^a-zA-Z0-9\\s]', '', text) for text in combined_text]\n",
        "\n",
        "# Step 2: Normalization\n",
        "normalized_text = [text.lower() for text in cleaned_text]\n",
        "\n",
        "# Step 3: Tokenization\n",
        "tokenized_text = [word_tokenize(text) for text in normalized_text]\n",
        "\n",
        "# Step 4: Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_text = [[lemmatizer.lemmatize(word) for word in tokens] for tokens in tokenized_text]\n",
        "\n",
        "# Step 5: Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_text = [[word for word in tokens if word not in stop_words] for tokens in lemmatized_text]\n",
        "\n",
        "# Print the processed text\n",
        "for text in filtered_text:\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "OatUBJ8pRhIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the list of lists into a single list of words\n",
        "all_words = [word for sublist in filtered_text for word in sublist]\n",
        "\n",
        "# Get unique words\n",
        "unique_words = set(all_words)\n",
        "\n",
        "# Print unique words\n",
        "print(\"Unique Words:\")\n",
        "for word in unique_words:\n",
        "    print(word)"
      ],
      "metadata": {
        "id": "-yw0KjbCTswV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "less_three = [word for word in unique_words if len(word) < 3]\n",
        "print(less_three)\n"
      ],
      "metadata": {
        "id": "yur_voWVOQCV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
